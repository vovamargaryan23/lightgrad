{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T21:00:51.938566105Z",
     "start_time": "2026-01-16T21:00:51.914556355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from core.structs import Tensor\n",
    "import numpy as np"
   ],
   "id": "e28afdcf6bbcb0d8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-16T21:00:53.481596506Z",
     "start_time": "2026-01-16T21:00:53.442439329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_linear_regression():\n",
    "    # 1. DATA GENERATION (Synthetic Data)\n",
    "    # We want the model to learn: y = 2x\n",
    "    X_raw = np.random.randn(100, 1)\n",
    "    y_raw = 2 * X_raw + (np.random.randn(100, 1) * 0.1) # Add some noise\n",
    "\n",
    "    # Wrap in your Tensor\n",
    "    X = Tensor(X_raw)\n",
    "    y = Tensor(y_raw)\n",
    "\n",
    "    # 2. MODEL INITIALIZATION\n",
    "    # A simple linear layer: y = xW + b\n",
    "    # Weights initialized randomly\n",
    "    W = Tensor(np.random.randn(1, 1), requires_grad=True)\n",
    "    b = Tensor(np.zeros((1,)), requires_grad=True)\n",
    "\n",
    "    print(f\"Initial Weight: {W.data[0][0]:.4f} (Target: 2.0)\")\n",
    "    print(f\"Initial Bias:   {b.data[0]:.4f} (Target: 0.0)\")\n",
    "\n",
    "    # 3. TRAINING LOOP\n",
    "    learning_rate = 0.01\n",
    "    for epoch in range(100):\n",
    "\n",
    "        # --- Forward Pass ---\n",
    "        # 1. Prediction\n",
    "        y_pred = X @ W + b\n",
    "\n",
    "        # 2. Loss (Mean Squared Error)\n",
    "        # Note: We don't have a generic MSE/Sum/Mean yet, so we write it manually:\n",
    "        # loss = sum((y_pred - y)^2) / N\n",
    "        diff = y_pred - y\n",
    "        loss = (diff * diff) # Element-wise square\n",
    "\n",
    "        # We need a 'sum' or 'mean' reduction for the scalar loss.\n",
    "        # Since your Tensor doesn't have .sum() yet, let's cheat and look at .grad\n",
    "        # from the perspective of the last tensor.\n",
    "        # Actually, let's implement a quick hack for Scalar Loss:\n",
    "        # We will set the gradient of 'loss' manually to be (1/N) to simulate mean.\n",
    "\n",
    "        # --- Backward Pass ---\n",
    "        # Zero Gradients (Manual for now)\n",
    "        W.grad = np.zeros_like(W.data)\n",
    "        b.grad = np.zeros_like(b.data)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # --- Update Step (SGD) ---\n",
    "        # The backward() sets gradients on 'diff * diff'.\n",
    "        # But wait, 'loss' is a tensor of shape (100, 1).\n",
    "        # We usually sum it to get a scalar.\n",
    "        # YOUR CURRENT BACKWARD sets grad=ones_like(data).\n",
    "        # So it acts like a Sum(). That works!\n",
    "\n",
    "        # W = W - lr * W.grad\n",
    "        W.data -= learning_rate * W.grad\n",
    "        b.data -= learning_rate * b.grad\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            # We assume sum of squared errors here\n",
    "            print(f\"Epoch {epoch} | Loss: {np.sum(loss.data):.4f} | W: {W.data[0][0]:.4f}\")\n",
    "\n",
    "    print(\"--- Final Result ---\")\n",
    "    print(f\"Learned Weight: {W.data[0][0]:.4f}\")\n",
    "    print(f\"Learned Bias:   {b.data[0]:.4f}\")\n",
    "\n",
    "# Run it\n",
    "train_linear_regression()"
   ],
   "id": "b3c8eede9b97f2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weight: 0.0302 (Target: 2.0)\n",
      "Initial Bias:   0.0000 (Target: 0.0)\n",
      "Epoch 0 | Loss: 434.2760 | W: 4.4003\n",
      "Epoch 10 | Loss: 62306.3214 | W: 28.5183\n",
      "Epoch 20 | Loss: 11189390.5028 | W: 355.1834\n",
      "Epoch 30 | Loss: 2009891432.4500 | W: 4734.5775\n",
      "Epoch 40 | Loss: 361026323648.7271 | W: 63429.5330\n",
      "Epoch 50 | Loss: 64849277098081.4844 | W: 850084.0017\n",
      "Epoch 60 | Loss: 11648537695678188.0000 | W: 11393155.5087\n",
      "Epoch 70 | Loss: 2092366122176123648.0000 | W: 152695799.1843\n",
      "Epoch 80 | Loss: 375840822565620875264.0000 | W: 2046492791.1647\n",
      "Epoch 90 | Loss: 67510328335794545098752.0000 | W: 27427950303.3817\n",
      "--- Final Result ---\n",
      "Learned Weight: -283568623777.7460\n",
      "Learned Bias:   158761015788.1501\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "c.backward()\n",
    "c.data"
   ],
   "id": "c05ba3f383f56d4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
